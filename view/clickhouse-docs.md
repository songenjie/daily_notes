

![BD4DDA0F-EC0B-44E1-9425-4A7E38A0D74A](/Users/songenjie/Library/Application Support/JDTimLine/songenjie/images/BD4DDA0F-EC0B-44E1-9425-4A7E38A0D74A.jpg)







![img](https://pic3.zhimg.com/80/67eecf7262927f728dd1c00aa9e0fe46_720w.png)





团队组成

1. 演讲型的选手
2. 重点把 放在 ppt上
3. 大数据与算法通道 未来可落地的 宏观的展望
4. 算法、痛点的优化





存储

1. 深度的列存储

2. 大宽表，每个模型也就是表包含了大量的列

3. 写入，数据总是以相当大的批次写入

4. 数据一致性 更高要求在数据可用性 容错性事务不是必须的，对数据一致性要求低

5. 较少甚至不修改数据

   



查询

1. 读大于写，且大部分是读请求，但是查询也是较少了 偏分析型的数据库 通常每台服务器每秒数百个查询或更少
2. 每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列
3. 处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行）
4. 查询结果明显小于源数据，换句话说，数据被过滤或聚合后能够被盛放在单台服务器的内存中
5. 查询更适应于只有一个大表的场景





1. 不支持Transaction：想快就别想Transaction
2. 聚合结果必须小于一台机器的内存大小：不是大问题
3. 缺少完整的Update/Delete操作
4. 支持有限操作系统



 The engine that uses the merge tree (see MergeTreeData) and replicated through ZooKeeper.

   ZooKeeper is used for the following things:
   - the structure of the table (/metadata, /columns)
   - action log with data (/log/log-...,/replicas/replica_name/queue/queue-...);
   - a replica list (/replicas), and replica activity tag (/replicas/replica_name/is_active), replica addresses (/replicas/replica_name/host);
   - select the leader replica (/leader_election) - this is the replica that assigns the merge;
   - a set of parts of data on each replica (/replicas/replica_name/parts);
   - list of the last N blocks of data with checksum, for deduplication (/blocks);
   - the list of incremental block numbers (/block_numbers) that we are about to insert,
     to ensure the linear order of data insertion and data merge only on the intervals in this sequence;
   - coordinates writes with quorum (/quorum).
   - Storage of mutation entries (ALTER DELETE, ALTER UPDATE etc.) to execute (/mutations).
     See comments in StorageReplicatedMergeTree::mutate() for details.

The replicated tables have a common log (/log/log-...).
   Log - a sequence of entries (LogEntry) about what to do.
   Each entry is one of:
   - normal data insertion (GET),
   - merge (MERGE),
   - delete the partition (DROP).

   Each replica copies (queueUpdatingTask, pullLogsToQueue) entries from the log to its queue (/replicas/replica_name/queue/queue-...)
    and then executes them (queueTask).
   Despite the name of the "queue", execution can be reordered, if necessary (shouldExecuteLogEntry, executeLogEntry).
   In addition, the records in the queue can be generated independently (not from the log), in the following cases:
   - when creating a new replica, actions are put on GET from other replicas (createReplica);
   - if the part is corrupt (removePartAndEnqueueFetch) or absent during the check (at start - checkParts, while running - searchForMissingPart),
     actions are put on GET from other replicas;

   The replica to which INSERT was made in the queue will also have an entry of the GET of this data.
   Such an entry is considered to be executed as soon as the queue handler sees it.

   The log entry has a creation time. This time is generated by the clock of server that created entry
   - the one on which the corresponding INSERT or ALTER query came.

   For the entries in the queue that the replica made for itself,
   as the time will take the time of creation the appropriate part on any of the replicas.


 There are three places for each part, where it should be
   1. In the RAM, data_parts, all_data_parts.
   2. In the filesystem (FS), the directory with the data of the table.
   3. in ZooKeeper (ZK).

   When adding a part, it must be added immediately to these three places.
   This is done like this
   - [FS] first write the part into a temporary directory on the filesystem;
   - [FS] rename the temporary part to the result on the filesystem;
   - [RAM] immediately afterwards add it to the `data_parts`, and remove from `data_parts` any parts covered by this one;
   - [RAM] also set the `Transaction` object, which in case of an exception (in next point),
     rolls back the changes in `data_parts` (from the previous point) back;
   - [ZK] then send a transaction (multi) to add a part to ZooKeeper (and some more actions);
   - [FS, ZK] by the way, removing the covered (old) parts from filesystem, from ZooKeeper and from `all_data_parts`
     is delayed, after a few minutes.

   There is no atomicity here.
   It could be possible to achieve atomicity using undo/redo logs and a flag in `DataPart` when it is completely ready.
   But it would be inconvenient - I would have to write undo/redo logs for each `Part` in ZK, and this would increase already large number of interactions.

   Instead, we are forced to work in a situation where at any time
    (from another thread, or after server restart), there may be an unfinished transaction.
    (note - for this the part should be in RAM)
   From these cases the most frequent one is when the part is already in the data_parts, but it's not yet in ZooKeeper.
   This case must be distinguished from the case where such a situation is achieved due to some kind of damage to the state.

   Do this with the threshold for the time.
   If the part is young enough, its lack in ZooKeeper will be perceived optimistically - as if it just did not have time to be added there
    - as if the transaction has not yet been executed, but will soon be executed.
   And if the part is old, its absence in ZooKeeper will be perceived as an unfinished transaction that needs to be rolled back.

   PS. Perhaps it would be better to add a flag to the DataPart that a part is inserted into ZK.
   But here it's too easy to get confused with the consistency of this flag.


使用合并树（请参阅MergeTreeData）并通过ZooKeeper复制的引擎。

   ZooKeeper用于以下用途：
   -表的结构（/元数据，/列）
   -带有数据的操作日志（/ log / log -...，/ replicas / replica_name / queue / queue -...）；
   -副本列表（/ replicas）和副本活动标签（/ replicas / replica_name / is_active），副本地址（/ replicas / replica_name / host）；
   -选择领导者副本（/ leader_election）-这是分配合并的副本；
   -每个副本上的一组数据部分（/ replicas / replica_name / parts）；
   -带有校验和的最后N个数据块的列表，用于重复数据删除（/ blocks）；
   -我们将要插入的增量块号（/ block_numbers）的列表，
     确保仅按此顺序的间隔进行数据插入和数据合并的线性顺序；
   -坐标以定额（/ quorum）写入。
   -存储要执行的突变项（ALTER DELETE，ALTER UPDATE等）。
     有关详细信息，请参见StorageReplicatedMergeTree :: mutate（）中的注释。

复制的表具有公共日志（/ log / log -...）。
   日志-有关操作的一系列输入（LogEntry）。
   每个条目是以下之一：
   -普通数据插入（GET），
   -合并（合并），
   -删除分区（DROP）。

   每个副本将日志中的条目（queueUpdatingTask，pullLogsToQueue）复制到其队列（/ replicas / replica_name / queue / queue -...）
    然后执行它们（queueTask）。
   尽管有“队列”的名称，但可以根据需要对执行进行重新排序（shouldExecuteLogEntry，executeLogEntry）。
   此外，在以下情况下，可以独立（而不是从日志）生成队列中的记录：
   -创建新副本时，将从其他副本（createReplica）对GET进行操作；
   -如果零件损坏（removePartAndEnqueueFetch）或在检查过程中不存在（在开始时-checkParts，在运行时-searchForMissingPart），
     从其他副本将操作放在GET上；

   队列中进行INSERT操作的副本也将具有此数据的GET条目。
   一旦队列处理程序看到该条目，便认为该条目已执行。

   日志条目具有创建时间。此时间是由创建条目的服务器的时钟生成的
   -相应的INSERT或ALTER查询所在的查询。

   对于副本为其自己制作的队列中的条目，
   因为时间将花费时间在任何副本上创建适当的部分。


 每个部分都有三个位置
   1.在RAM中，data_parts，all_data_parts。
   2.在文件系统（FS）中，包含表数据的目录。
   3.在ZooKeeper（ZK）中。

   添加零件时，必须立即将其添加到这三个位置。
   这样完成
   -[FS]首先将部件写入文件系统上的临时目录中；
   -[FS]将临时部分重命名为文件系统上的结果；
   -[RAM]之后立即将其添加到“ data_parts”中，并从“ data_parts”中删除此部分涵盖的任何部分；
   -[RAM]还设置了“ Transaction”对象，如果发生异常（在下一点），
     回滚data_parts中的更改（从上一点开始）；
   -[ZK]然后发送交易（多次）以将零件添加到ZooKeeper（以及更多其他操作）；
   -[FS，ZK]顺便说一下，从文件系统，ZooKeeper和`all_data_parts`中删除覆盖的（旧）部分。
     几分钟后被延迟。

   这里没有原子性。
   完全准备就绪时，可以使用撤消/重做日志和DataPart中的标志来实现原子性。
   但这很不方便-我将不得不为ZK中的每个“零件”编写撤消/重做日志，这将增加大量的交互。

   相反，我们被迫在任何时候
    （从另一个线程，或在服务器重新启动之后），可能有未完成的事务。
    （注意-为此，该部分应该在RAM中）
   在这些情况下，最常见的情况是该零件已经在data_parts中，但是在ZooKeeper中还没有。
   这种情况必须与由于某种形式的国家损害而达到这种情况的情况区分开。

   使用时间阈值执行此操作。
   如果零件足够年轻，则会乐观地看到其缺乏ZooKeeper-好像没有时间在此添加零件
    -好像尚未执行事务，但是将很快执行。
   而且，如果零件很旧，则它在ZooKeeper中的缺失将被视为未完成的事务，需要回滚。

   PS。也许最好在DataPart中添加一个标记，以将零件插入到ZK中。
   但是，在这里容易混淆此标志的一致性。







```c++
template<class _Tp>
class _LIBCPP_TEMPLATE_VIS enable_shared_from_this
{
    mutable weak_ptr<_Tp> __weak_this_;
protected:
    _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR
    enable_shared_from_this() _NOEXCEPT {}
    _LIBCPP_INLINE_VISIBILITY
    enable_shared_from_this(enable_shared_from_this const&) _NOEXCEPT {}
    _LIBCPP_INLINE_VISIBILITY
    enable_shared_from_this& operator=(enable_shared_from_this const&) _NOEXCEPT
        {return *this;}
    _LIBCPP_INLINE_VISIBILITY
    ~enable_shared_from_this() {}
public:
    _LIBCPP_INLINE_VISIBILITY
    shared_ptr<_Tp> shared_from_this()
        {return shared_ptr<_Tp>(__weak_this_);}
    _LIBCPP_INLINE_VISIBILITY
    shared_ptr<_Tp const> shared_from_this() const
        {return shared_ptr<const _Tp>(__weak_this_);}
#if _LIBCPP_STD_VER > 14
    _LIBCPP_INLINE_VISIBILITY
    weak_ptr<_Tp> weak_from_this() _NOEXCEPT
       { return __weak_this_; }
    _LIBCPP_INLINE_VISIBILITY
    weak_ptr<const _Tp> weak_from_this() const _NOEXCEPT
        { return __weak_this_; }
#endif // _LIBCPP_STD_VER > 14
    template <class _Up> friend class shared_ptr;
};

```





bufferio 





- partition.dat: 分区信息
- checksum.txt: 数据校验信息
- columns.txt: 列信息
- count.txt: 计数信息
- primary.idx: 一级索引信息，用于存储稀疏索引信息
- [column].bin: 存储某一列的信息，默认使用lz4压缩算法存储
- [column].mrk: 列字段标记问题，保存.bin文件中数据的偏移量信息
- [column].mrk2: 如果定义了自适应索引，则会出现该文件，作用和.mrk文件一样
- partition.dat、minmax_[column].idx: 定义了分区键，会出现这二个文件，partition存储当前分区下分区表达式最终生成的值，minmax_[column].idx记录当前分区下对应原始数据的最小最大值
- skp_idx_[Column].idx与skp_idx_[Column].mrk: 二级索引信息





100p

高吞吐

低延时











## 创建方式和存储结构

Mergetree在写入数据时，数据总会以数据片段的形式写入磁盘，为了避免片段过多，ClickHouse会通过后台线程，定期合并这些数据片段，属于相同分区的数据片段会被合并成一个新的片段，正式合并树名称的由来。

### 创建方式

```
CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
(
    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
    ...
    INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1,
    INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2
) ENGINE = MergeTree()
[PARTITION BY expr]
[ORDER BY expr]
[PRIMARY KEY expr]
[SAMPLE BY expr]
[SETTINGS name=value, ...]
```

主要参数：

#### PARTITION BY

分区键，不声明分区键，则会默认生成一个名为all的分区。

##### ORDER BY

`必填`，排序键，默认情况下主键与排序键相同。

#### PRIMARY KEY

会根据主键字段生成一级索引，用于加速查询，可不声明，默认是ORDER BY定义的字段。

#### SAMPLE BY

抽样表达式，声明数据以何种标准进行采样，如果使用此配置，必须子主键的配置中也声明同样的表达式。
ORDER BY (CounterID,intHash32(UserID))
SAMPLE BY intHash32(UserID)

#### SETTINGS

index_granularity:索引粒度，默认8192，也就每隔8192行才生成一条索引
enable_mixed_granularity_parts:是否开启自适应索引间隔功能，默认开启
index_granularity_bytes:索引粒度，根据每一批次写入数据的大小，动态划分间隔大小，默认10M(`10*1024*1024`)

### 存储结构

创建测试表

```
CREATE TABLE test.part_v1
(
    `ID` String,
    `URL` String,
    `age` UInt8 DEFAULT 0,
    `EventTime` Date
)
ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(EventTime)
ORDER BY ID
SETTINGS index_granularity = 8192
```

插入数据

```
insert into test.part_v1 values
('A001', 'www.test1.com', 1,'2020-08-01')
('A001', 'www.test1.com', 1,'2020-08-02')
('A002', 'www.test1.com', 1,'2020-08-03');
```

查看目录结构

```
[root@test 20200801_12_12_0]# ll
总用量 56
-rw-r----- 1 clickhouse clickhouse  29 8月  18 15:56 age.bin
-rw-r----- 1 clickhouse clickhouse  48 8月  18 15:56 age.mrk2
-rw-r----- 1 clickhouse clickhouse 456 8月  18 15:56 checksums.txt
-rw-r----- 1 clickhouse clickhouse  91 8月  18 15:56 columns.txt
-rw-r----- 1 clickhouse clickhouse   1 8月  18 15:56 count.txt
-rw-r----- 1 clickhouse clickhouse  32 8月  18 15:56 EventTime.bin
-rw-r----- 1 clickhouse clickhouse  48 8月  18 15:56 EventTime.mrk2
-rw-r----- 1 clickhouse clickhouse  42 8月  18 15:56 ID.bin
-rw-r----- 1 clickhouse clickhouse  48 8月  18 15:56 ID.mrk2
-rw-r----- 1 clickhouse clickhouse   4 8月  18 15:56 minmax_EventTime.idx
-rw-r----- 1 clickhouse clickhouse   4 8月  18 15:56 partition.dat
-rw-r----- 1 clickhouse clickhouse  10 8月  18 15:56 primary.idx
-rw-r----- 1 clickhouse clickhouse  49 8月  18 15:56 URL.bin
-rw-r----- 1 clickhouse clickhouse  48 8月  18 15:56 URL.mrk2
[root@test 20200801_12_12_0]# pwd
/var/lib/clickhouse/data/test/part_v1/20200801_12_12_0
```

目录层次：数据库名 > 数据表名 > 分区目录 > 分区下具体文件
20200801_12_12_0是分区名

.txt是明文存储，.bin/.dex/.mrk二进制存储

- partition.dat: 分区信息
- checksum.txt: 数据校验信息
- columns.txt: 列信息
- count.txt: 计数信息
- primary.idx: 一级索引信息，用于存储稀疏索引信息
- [column].bin: 存储某一列的信息，默认使用lz4压缩算法存储
- [column].mrk: 列字段标记问题，保存.bin文件中数据的偏移量信息
- [column].mrk2: 如果定义了自适应索引，则会出现该文件，作用和.mrk文件一样
- partition.dat、minmax_[column].idx: 定义了分区键，会出现这二个文件，partition存储当前分区下分区表达式最终生成的值，minmax_[column].idx记录当前分区下对应原始数据的最小最大值
- skp_idx_[Column].idx与skp_idx_[Column].mrk: 二级索引信息





## 数据分区

### 数据的分区规则

分区规则由分区ID决定，，分区ID生成规则有四种逻辑

- 不指定分区键：没有定义PARTITION BY，分区ID默认all
- 使用整型：直接按该整型的字符串形式输出，做为分区ID
- 使用日期类型：分区键时日期类型，或者可以转化成日期类型，比如用today转化，YYYYMMDD格式按天分区，YYYYMM按月分区等
- 使用其他类型：String、Float类型等，通过128位的Hash算法取其Hash值作为分区ID

![image.png](https://segmentfault.com/img/bVbLXJh)

数据进行分区存储，在查询时可以快速定位数据位置

### 分区目录的命名规则

分区命名规则，对于20200801_1_1_0
PartitionID_MinBlockNum_MaxBlockNum_Level

- PartitionID: 分区ID，20200801就是分区ID
- MinBlockNum、MaxBlockNum: 最小分区块编号和最大分区块编号，BlockNum是整型的自增长编号，从1开始，新创建一个分区目录时，会+1，新创建的分区MinBlockNum=MaxBlockNum
- Level：合并的层级，被合并的次数

### 分区目录的合并过程

每次数据insert写入，都会生成新的分区目录，在之后的某个时刻（写入后的10-15分钟，也可以手动执行optimize强制合并）会通过后台任务再将属于相同分区的多个目录合并成一个新的目录，已经存在的目录通过后台任务删除（默认8分钟）。

合并之后新目录名规则：

- MinBlockNum：取同一分区内所有目录中最小的MinBlockNum值
- MaxBlockNum：取同一分区内所有目录中最大的MaxBlockNum值
- Level：取同以分区内最大Level值并+1

![image.png](https://segmentfault.com/img/bVbLXJq)

## 一级索引

一级索引也就是主键索引，通过PRIMARY KEY/ORDER BY定义
会写入primary.idx文件中

![image.png](https://segmentfault.com/img/bVbLXJL)

稀疏索引和稠密索引的区别

稀疏索引使用一个索引标记一大段时间，减少了索引的数据量，使得primary.idx可以常驻内存，加速数据查询

### 数据索引的生成过程

PARTITION BYtoYYYYMM(EventDate)），所以2014年3月份的数据最终会被划分到同一个分区目录内。使用CounterID作为主键（ORDER BY CounterID），每间隔8192行会生成一个主键索引保存到primary.idx文件中
![image.png](https://segmentfault.com/img/bVbLXKc)

压缩数据块
![image.png](https://segmentfault.com/img/bVbLXJ4)

### 数据标记的生成规则

数据标记是衔接一级索引和数据的桥梁
![image.png](https://segmentfault.com/img/bVbLXKx)

数据标记和索引区间是对齐的，均按照index_granularity的粒度间隔。只需简单通过索引区间的下标编号就可以直接找到对应的数据标记。每一个列字段[Column].bin文件都有一个与之对应的[Column].mrk数据标记文件，用于记录数据在．bin文件中的偏移量信息
![image.png](https://segmentfault.com/img/bVbLXKL)

一行标记数据使用一个元组表示，元组内包含两个整型数值的偏移量信息。对应的.bin压缩文件中，压缩数据块的起始偏移量；以及将该数据压缩块解压后，其未压缩数据的起始偏移量
每一行标记数据都表示了一个片段的数据（默认8192行）在.bin压缩文件中的读取位置信息。标记数据与一级索引数据不同，它并不能常驻内存，而是使用LRU（最近最少使用）缓存策略加快其取用速度。





bitshuffler 

### 分区、索引、标记和压缩数据的协同总结

#### 写入过程

首先生成分区目录，属于相同分区的目录会依照规则合并到一起
紧接着按照index_granularity索引粒度，会分别生成primary.idx一级索引（如果声明了二级索引，还会创建二级索引文件）、每一个列字段的．mrk数据标记和．bin压缩数据文件
![image](https://segmentfault.com/img/bVbLXNH)

#### 查询过程

查询的本质，可以看作一个不断减小数据范围的过程。在最理想的情况下，MergeTree首先可以依次借助分区索引、一级索引和二级索引，将数据扫描范围缩至最小。然后再借助数据标记，将![image](https://segmentfault.com/img/bVbLXNr)需要解压与计算的数据范围缩至最小

如果一条查询语句用不到索引会进行分区目标扫描，虽不能缩小数据范围，但是MergeTree仍然能够借助数据标记，以多线程的形式同时读取多个压缩数据块，以提升性能



#### 数据标记和压缩数据块的对应关系

每个压缩数据块的体积都被严格控制在64KB～1MB。而一个间隔（index_granularity）的数据，又只会产生一行数据标记，根据一个间隔内数据的实际字节大小，数据标记和压缩数据块之间会产生三种不同的对应关系



##### 多对一

多个数据标记对应一个压缩数据块，当一个间隔（index_granularity）内的数据未压缩大小size小于64KB时，会出现这种对应关系。
![image](https://segmentfault.com/img/bVbLXMm)



##### 一对一

一个数据标记对应一个压缩数据块，当一个间隔（index_granularity）内的数据未压缩大小size大于64KB小于1M时，会出现这种对应关系。
![image](https://segmentfault.com/img/bVbLXMx)



##### 一对多

一个数据标记对应多个压缩数据块，当一个间隔（index_granularity）内的数据未压缩大小size大于1M时，会出现这种对应关系。
![image](https://segmentfault.com/img/bVbLXM6)

## 二级索引

二级索引又称跳数索引，由数据的聚合信息构建而成，根据索引类型的不同，其聚合信息的内容也不同。
需要在CREATE语句内定义，定义了跳数索引会额外生成相应的索引文件后标记文件
skp_idx_[Column].idx和skp_idx_[Column].mrk

1.定义表结构

```
CREATE TABLE test.part_v2
(
    `ID` String,
    `URL` String,
    `age` UInt8 DEFAULT 0,
    `EventTime` Date
)
ENGINE = MergeTree()
    PARTITION BY toYYYYMMDD(EventTime)
    ORDER BY ID
    SETTINGS index_granularity = 8192
```

查看文件目录

```
总用量 4
drwxr-x--- 2 clickhouse clickhouse 6 8月  20 18:54 detached
-rw-r----- 1 clickhouse clickhouse 1 8月  20 18:54 format_version.txt
```











column storage



列存 

索引的索引

每一列数据 排序 字符串 字典编码



compact min batch





事实引擎

压缩 历史数据索引

实时索引



列压缩 nblock 

parquet 



Insert delete bit set ,compact 

Update 主键更新





k v 



simd 指令



avg min max 



Adb 2-3 熵加 减小

rollup 表 





1 榨干系统 driect io page cache

2 simd 指令

3 cpu cache 

jvm 自动 simd



4 多核 

5 ssd 

nvme cache 

Inter aep 







doris clickhouse

aep 

