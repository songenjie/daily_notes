 The engine that uses the merge tree (see MergeTreeData) and replicated through ZooKeeper.

   ZooKeeper is used for the following things:
   - the structure of the table (/metadata, /columns)
   - action log with data (/log/log-...,/replicas/replica_name/queue/queue-...);
   - a replica list (/replicas), and replica activity tag (/replicas/replica_name/is_active), replica addresses (/replicas/replica_name/host);
   - select the leader replica (/leader_election) - this is the replica that assigns the merge;
   - a set of parts of data on each replica (/replicas/replica_name/parts);
   - list of the last N blocks of data with checksum, for deduplication (/blocks);
   - the list of incremental block numbers (/block_numbers) that we are about to insert,
     to ensure the linear order of data insertion and data merge only on the intervals in this sequence;
   - coordinates writes with quorum (/quorum).
   - Storage of mutation entries (ALTER DELETE, ALTER UPDATE etc.) to execute (/mutations).
     See comments in StorageReplicatedMergeTree::mutate() for details.

The replicated tables have a common log (/log/log-...).
   Log - a sequence of entries (LogEntry) about what to do.
   Each entry is one of:
   - normal data insertion (GET),
   - merge (MERGE),
   - delete the partition (DROP).

   Each replica copies (queueUpdatingTask, pullLogsToQueue) entries from the log to its queue (/replicas/replica_name/queue/queue-...)
    and then executes them (queueTask).
   Despite the name of the "queue", execution can be reordered, if necessary (shouldExecuteLogEntry, executeLogEntry).
   In addition, the records in the queue can be generated independently (not from the log), in the following cases:
   - when creating a new replica, actions are put on GET from other replicas (createReplica);
   - if the part is corrupt (removePartAndEnqueueFetch) or absent during the check (at start - checkParts, while running - searchForMissingPart),
     actions are put on GET from other replicas;

   The replica to which INSERT was made in the queue will also have an entry of the GET of this data.
   Such an entry is considered to be executed as soon as the queue handler sees it.

   The log entry has a creation time. This time is generated by the clock of server that created entry
   - the one on which the corresponding INSERT or ALTER query came.

   For the entries in the queue that the replica made for itself,
   as the time will take the time of creation the appropriate part on any of the replicas.


 There are three places for each part, where it should be
   1. In the RAM, data_parts, all_data_parts.
   2. In the filesystem (FS), the directory with the data of the table.
   3. in ZooKeeper (ZK).

   When adding a part, it must be added immediately to these three places.
   This is done like this
   - [FS] first write the part into a temporary directory on the filesystem;
   - [FS] rename the temporary part to the result on the filesystem;
   - [RAM] immediately afterwards add it to the `data_parts`, and remove from `data_parts` any parts covered by this one;
   - [RAM] also set the `Transaction` object, which in case of an exception (in next point),
     rolls back the changes in `data_parts` (from the previous point) back;
   - [ZK] then send a transaction (multi) to add a part to ZooKeeper (and some more actions);
   - [FS, ZK] by the way, removing the covered (old) parts from filesystem, from ZooKeeper and from `all_data_parts`
     is delayed, after a few minutes.

   There is no atomicity here.
   It could be possible to achieve atomicity using undo/redo logs and a flag in `DataPart` when it is completely ready.
   But it would be inconvenient - I would have to write undo/redo logs for each `Part` in ZK, and this would increase already large number of interactions.

   Instead, we are forced to work in a situation where at any time
    (from another thread, or after server restart), there may be an unfinished transaction.
    (note - for this the part should be in RAM)
   From these cases the most frequent one is when the part is already in the data_parts, but it's not yet in ZooKeeper.
   This case must be distinguished from the case where such a situation is achieved due to some kind of damage to the state.

   Do this with the threshold for the time.
   If the part is young enough, its lack in ZooKeeper will be perceived optimistically - as if it just did not have time to be added there
    - as if the transaction has not yet been executed, but will soon be executed.
   And if the part is old, its absence in ZooKeeper will be perceived as an unfinished transaction that needs to be rolled back.

   PS. Perhaps it would be better to add a flag to the DataPart that a part is inserted into ZK.
   But here it's too easy to get confused with the consistency of this flag.


使用合并树（请参阅MergeTreeData）并通过ZooKeeper复制的引擎。

   ZooKeeper用于以下用途：
   -表的结构（/元数据，/列）
   -带有数据的操作日志（/ log / log -...，/ replicas / replica_name / queue / queue -...）；
   -副本列表（/ replicas）和副本活动标签（/ replicas / replica_name / is_active），副本地址（/ replicas / replica_name / host）；
   -选择领导者副本（/ leader_election）-这是分配合并的副本；
   -每个副本上的一组数据部分（/ replicas / replica_name / parts）；
   -带有校验和的最后N个数据块的列表，用于重复数据删除（/ blocks）；
   -我们将要插入的增量块号（/ block_numbers）的列表，
     确保仅按此顺序的间隔进行数据插入和数据合并的线性顺序；
   -坐标以定额（/ quorum）写入。
   -存储要执行的突变项（ALTER DELETE，ALTER UPDATE等）。
     有关详细信息，请参见StorageReplicatedMergeTree :: mutate（）中的注释。

复制的表具有公共日志（/ log / log -...）。
   日志-有关操作的一系列输入（LogEntry）。
   每个条目是以下之一：
   -普通数据插入（GET），
   -合并（合并），
   -删除分区（DROP）。

   每个副本将日志中的条目（queueUpdatingTask，pullLogsToQueue）复制到其队列（/ replicas / replica_name / queue / queue -...）
    然后执行它们（queueTask）。
   尽管有“队列”的名称，但可以根据需要对执行进行重新排序（shouldExecuteLogEntry，executeLogEntry）。
   此外，在以下情况下，可以独立（而不是从日志）生成队列中的记录：
   -创建新副本时，将从其他副本（createReplica）对GET进行操作；
   -如果零件损坏（removePartAndEnqueueFetch）或在检查过程中不存在（在开始时-checkParts，在运行时-searchForMissingPart），
     从其他副本将操作放在GET上；

   队列中进行INSERT操作的副本也将具有此数据的GET条目。
   一旦队列处理程序看到该条目，便认为该条目已执行。

   日志条目具有创建时间。此时间是由创建条目的服务器的时钟生成的
   -相应的INSERT或ALTER查询所在的查询。

   对于副本为其自己制作的队列中的条目，
   因为时间将花费时间在任何副本上创建适当的部分。


 每个部分都有三个位置
   1.在RAM中，data_parts，all_data_parts。
   2.在文件系统（FS）中，包含表数据的目录。
   3.在ZooKeeper（ZK）中。

   添加零件时，必须立即将其添加到这三个位置。
   这样完成
   -[FS]首先将部件写入文件系统上的临时目录中；
   -[FS]将临时部分重命名为文件系统上的结果；
   -[RAM]之后立即将其添加到“ data_parts”中，并从“ data_parts”中删除此部分涵盖的任何部分；
   -[RAM]还设置了“ Transaction”对象，如果发生异常（在下一点），
     回滚data_parts中的更改（从上一点开始）；
   -[ZK]然后发送交易（多次）以将零件添加到ZooKeeper（以及更多其他操作）；
   -[FS，ZK]顺便说一下，从文件系统，ZooKeeper和`all_data_parts`中删除覆盖的（旧）部分。
     几分钟后被延迟。

   这里没有原子性。
   完全准备就绪时，可以使用撤消/重做日志和DataPart中的标志来实现原子性。
   但这很不方便-我将不得不为ZK中的每个“零件”编写撤消/重做日志，这将增加大量的交互。

   相反，我们被迫在任何时候
    （从另一个线程，或在服务器重新启动之后），可能有未完成的事务。
    （注意-为此，该部分应该在RAM中）
   在这些情况下，最常见的情况是该零件已经在data_parts中，但是在ZooKeeper中还没有。
   这种情况必须与由于某种形式的国家损害而达到这种情况的情况区分开。

   使用时间阈值执行此操作。
   如果零件足够年轻，则会乐观地看到其缺乏ZooKeeper-好像没有时间在此添加零件
    -好像尚未执行事务，但是将很快执行。
   而且，如果零件很旧，则它在ZooKeeper中的缺失将被视为未完成的事务，需要回滚。

   PS。也许最好在DataPart中添加一个标记，以将零件插入到ZK中。
   但是，在这里容易混淆此标志的一致性。







```c++
template<class _Tp>
class _LIBCPP_TEMPLATE_VIS enable_shared_from_this
{
    mutable weak_ptr<_Tp> __weak_this_;
protected:
    _LIBCPP_INLINE_VISIBILITY _LIBCPP_CONSTEXPR
    enable_shared_from_this() _NOEXCEPT {}
    _LIBCPP_INLINE_VISIBILITY
    enable_shared_from_this(enable_shared_from_this const&) _NOEXCEPT {}
    _LIBCPP_INLINE_VISIBILITY
    enable_shared_from_this& operator=(enable_shared_from_this const&) _NOEXCEPT
        {return *this;}
    _LIBCPP_INLINE_VISIBILITY
    ~enable_shared_from_this() {}
public:
    _LIBCPP_INLINE_VISIBILITY
    shared_ptr<_Tp> shared_from_this()
        {return shared_ptr<_Tp>(__weak_this_);}
    _LIBCPP_INLINE_VISIBILITY
    shared_ptr<_Tp const> shared_from_this() const
        {return shared_ptr<const _Tp>(__weak_this_);}
#if _LIBCPP_STD_VER > 14
    _LIBCPP_INLINE_VISIBILITY
    weak_ptr<_Tp> weak_from_this() _NOEXCEPT
       { return __weak_this_; }
    _LIBCPP_INLINE_VISIBILITY
    weak_ptr<const _Tp> weak_from_this() const _NOEXCEPT
        { return __weak_this_; }
#endif // _LIBCPP_STD_VER > 14
    template <class _Up> friend class shared_ptr;
};

```

